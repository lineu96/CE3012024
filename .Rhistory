q <- length(beta_hat)
# Gerar um valor de uma distribuição chi-quadrado
X2 <- rchisq(n = 1, df = n - q)
# Estimar sigma quadrado
sigma2_tilde <- sum(e_hat^2) / (n-q)
# Gerar uma amostra da distribuição normal multivariada dos coeficientes
beta_tilde <- MASS::mvrnorm(n = 1, mu = beta_hat, Sigma = SIGMA)
# Gerar valores de resíduos para os dados imputados
e_tilde <- rnorm(n, mean = 0, sd = sigma_hat)
# Calcular os valores imputados para X1.miss
X_imp <- beta_tilde[1] + beta_tilde[2] * dados$t.obs + beta_tilde[3] * dados$delta + dados$Z*beta_tilde[4] + e_tilde
# Substituir os valores imputados nos locais onde os valores de X1.miss
# estavam ausentes
dados$X[R == 0] <- X_imp[R == 0]
# Armazenar o conjunto de dados imputado na lista
conjuntos_dados_imputados[[k]] <- dados
}
return(conjuntos_dados_imputados)
}
imp.cox <- function(data) {
dados <- data
# IMPLEMENTAÇÃO CONGENIAL MODEL
m.cox <- coxph(Surv(t.obs, delta) ~ X + Z, data = dados)
# Extrair os coeficientes
beta_hat <- coef(m.cox)
omega_hat <- vcov(m.cox)
beta_tilde <- mvrnorm(n = 1, mu = beta_hat, Sigma = omega_hat)
# Estimar H_0(t)
ests <- basehaz(m.cox, centered = FALSE)
H0_t_hat <- ests$hazard
h0_t_hat <- exp(diff(ests[, 1])*diff(ests[, 2]))
# 5) Proposta f(.)
proposta <- dados
# 6) Aceitar ou rejeitar a proposta para cada conjunto imputado
K <- 5
conjuntos_dados_imputados <- imp.norm(dados)
# Inicializar lista para armazenar os dados aceitos
dados_aceitos <- list()
for (k in 1:K) {
dados <- data # Resetar os dados para o estado inicial
dados_imputados <- conjuntos_dados_imputados[[k]]
for (i in 1:nrow(dados)) {
if (is.na(dados$X[i])) {
if (dados$delta[i] == 0) { # para indivíduo censurado
p_aceitacao <- h0_t_hat*exp(-(1+log(H0_t_hat)))
} else { # indivíduo não censurado
p_aceitacao <- -H0_t_hat*(exp(proposta[i]*beta_tilde[1] + dados$Z[i]*beta_tilde[2]))
}
p_aceitacao <- as.numeric(p_aceitacao)
for (j in 1:length(p_aceitacao)) { # Iterar sobre cada elemento de p_aceitacao
if (runif(1) <= p_aceitacao[j]) {
dados$X[i] <- dados_imputados$X[i]
}
}
}
}
dados_aceitos[[k]] <- dados
}
return(dados_aceitos)
}
# A função with não funciona para os meus dados, criei essa
my_with <- function(conjuntos_dados_imputados) {
modelos <- list()
for (i in seq_along(conjuntos_dados_imputados)) {
dados_imputados <- conjuntos_dados_imputados[[i]]
modelo <- coxph(Surv(t.obs, delta) ~ X + Z, data = dados_imputados)
modelos[[i]] <- modelo
}
return(modelos)
}
# Simulando dados
theta_real <- 1 # verdadeiro valor do parâmetro
n <- 1000 # amostras
X <- rnorm(n, 0, 1) # X completo
Z <- rbinom(n, 1, 0.5) # Z
lambda <- exp(theta_real * X + theta_real * Z) # True Hazard
t <- rexp(n, rate = lambda) # Tempo de evento
C <- rweibull(n, shape = 1/2, scale = 1) # Tempo de censura
prop <- (1 - 1 / (1 + exp(2 * t))) # Verdadeiro PS
t.obs <- pmin(t, C)
# Indicador de missing
R <- rbinom(n, 1, prop)
# Indicador de censura
delta <- as.numeric(t <= C)
# NA baseado em R = 0,
X.miss <- ifelse(R == 0, NA, X)
# FULL
dados_comp <- data.frame(t.obs, delta, X, Z)
head(dados_comp)
# COM NA
dados <- data.frame(t.obs, delta, X.miss, Z)
head(dados)
colnames(dados) <- colnames(dados_comp)
# CASO COMPLETO (CC)
dados_cc <- na.omit(dados)
colnames(dados_cc) <- colnames(dados_comp)
# Modelo completo
m.full <- coxph(Surv(t.obs, delta) ~ X + Z, data = dados_comp)
print(summary(m.full)$coef[, c(1, 3)])
# Modelo CC (complete cases)
m.cc <- coxph(Surv(t.obs, delta) ~ X + Z, data = dados_cc)
print(summary(m.cc)$coef[, c(1, 3)])
# aplicando imp.cox
dados.imp.cox <- imp.cox(dados)
m.imp <- my_with(dados.imp.cox)
summary(pool(m.imp))[,c(2,3)]
library(smcfcs)
h0_t_hat
#-------------------------------------------------------------------------------
# Bibliotecas
library(survival)
library(tidyverse)
library(mice)
# Simulando dados
theta_real <- 1 # verdadeiro valor do parâmetro
n <- 1000 # amostras
X <- rnorm(n, 0, 1) # X completo
Z <- rbinom(n, 1, 0.5) # Z
lambda <- exp(theta_real * X + theta_real * Z) # True Hazard
t <- rexp(n, rate = lambda) # Tempo de evento
C <- rweibull(n, shape = 1/2, scale = 1) # Tempo de censura
prop <- (1 - 1 / (1 + exp(2 * t))) # Verdadeiro PS
t.obs <- pmin(t, C)
# Indicador de missing
R <- rbinom(n, 1, prop)
# Indicador de censura
delta <- as.numeric(t <= C)
# NA baseado em R = 0,
X.miss <- ifelse(R == 0, NA, X)
# FULL
dados_comp <- data.frame(t.obs, delta, X, Z)
head(dados_comp)
# COM NA
dados <- data.frame(t.obs, delta, X.miss, Z)
imps_bart <- smcfcs(dados, smtype="coxph", smformula="Surv(t.obs,delta)~ X + Z",
method = c("","", "norm",""), rjlimit = 1500)
impobj <- imputationList(imps_bart$impDatasets)
models.bart <- with(impobj, coxph(Surv(t.obs, delta) ~ X + Z))
aux.bart <- summary(MIcombine(models.bart))[,c(1,2)]
aux.bart
#-------------------------------------------------------------------------------
# Bibliotecas
library(survival)
library(tidyverse)
library(mice)
#-------------------------------------------------------------------------------
# Simulando dados
theta_real <- 1 # verdadeiro valor do parâmetro
n <- 1000 # amostras
X <- rnorm(n, 0, 1) # X completo
Z <- rbinom(n, 1, 0.5) # Z
lambda <- exp(theta_real * X + theta_real * Z) # True Hazard
t <- rexp(n, rate = lambda) # Tempo de evento
C <- rweibull(n, shape = 1/2, scale = 1) # Tempo de censura
prop <- (1 - 1 / (1 + exp(2 * t))) # Verdadeiro PS
t.obs <- pmin(t, C)
# Indicador de missing
R <- rbinom(n, 1, prop)
# Indicador de censura
delta <- as.numeric(t <= C)
# NA baseado em R = 0,
X.miss <- ifelse(R == 0, NA, X)
# FULL
dados_comp <- data.frame(t.obs, delta, X, Z)
head(dados_comp)
# COM NA
dados <- data.frame(t.obs, delta, X.miss, Z)
head(dados)
colnames(dados) <- colnames(dados_comp)
# CASO COMPLETO (CC)
dados_cc <- na.omit(dados)
colnames(dados_cc) <- colnames(dados_comp)
# Modelo FULL
m.full <- coxph(Surv(t.obs, delta) ~ X + Z, data = dados_comp)
summary(m.full)$coef[, c(1, 3)]
# Modelo CC (complete cases)
m.cc <- coxph(Surv(t.obs, delta) ~ X + Z, data = dados_cc)
summary(m.cc)$coef[, c(1, 3)]
# CONGENIAL MODEL BARTLLET
library(smcfcs)
library(mitools)
imps_bart <- smcfcs(dados, smtype="coxph", smformula="Surv(t.obs,delta)~ X + Z",
method = c("","", "norm",""), rjlimit = 1500)
impobj <- imputationList(imps_bart$impDatasets)
models.bart <- with(impobj, coxph(Surv(t.obs, delta) ~ X + Z))
aux.bart <- summary(MIcombine(models.bart))[,c(1,2)]
aux.bart
summary(m.full)$coef[, c(1, 3)]
summary(m.cc)$coef[, c(1, 3)]
aux.bart
# 1º passo é escolher os valores para o missing, vou escolher "norm"
# que é a minha implementação univariada com modificações
imp.norm <- function(data, K = 5) {
# Lista para armazenar os conjuntos de dados imputados
conjuntos_dados_imputados <- list()
for (k in 1:K) {
# Copiar os dados originais
dados <- data
# Identificar os índices dos valores ausentes em X1.miss
R <- 1 * !is.na(dados$X)
# Número de observações
n <- nrow(dados)
# Ajustar o modelo de regressão linear
m.0 <- lm(X ~ t.obs + delta +  Z, data = dados)
s <- summary(m.0)
sigma_hat <- s$sigma
# Coeficientes estimados
beta_hat <- coef(m.0)
# Matriz de covariância dos coeficientes estimados
SIGMA <- vcov(m.0)
# Resíduos do modelo
e_hat <- resid(m.0)
# Número de parâmetros
q <- length(beta_hat)
# Gerar um valor de uma distribuição chi-quadrado
X2 <- rchisq(n = 1, df = n - q)
# Estimar sigma quadrado
sigma2_tilde <- sum(e_hat^2) / (n-q)
# Gerar uma amostra da distribuição normal multivariada dos coeficientes
beta_tilde <- MASS::mvrnorm(n = 1, mu = beta_hat, Sigma = SIGMA)
# Gerar valores de resíduos para os dados imputados
e_tilde <- rnorm(n, mean = 0, sd = sigma_hat)
# Calcular os valores imputados para X1.miss
X_imp <- beta_tilde[1] + beta_tilde[2] * dados$t.obs + beta_tilde[3] * dados$delta + dados$Z*beta_tilde[4] + e_tilde
# Substituir os valores imputados nos locais onde os valores de X1.miss
# estavam ausentes
dados$X[R == 0] <- X_imp[R == 0]
# Armazenar o conjunto de dados imputado na lista
conjuntos_dados_imputados[[k]] <- dados
}
return(conjuntos_dados_imputados)
}
# aplicando no banco de dados
imp.norm(dados)
# aplicando no banco de dados
imp_0 <- imp.norm(dados)
imp_0
# aplicando no banco de dados
imp_0 <- imp.norm(dados, K = 1)
imp_0
# aplicando no banco de dados
imp_0 <- imp.norm(dados)
# aplicando no banco de dados
imp_0 <- imp.norm(dados)
imp_0
# agora com esses dados precisamos ajustar o modelo de Cox
m.cox <- list()
# agora com esses dados precisamos ajustar o modelo de Cox
m.cox <- list()
for(i in seq_along(imp_0)){
dados_imp <- imp_0[[i]]
models <- coxph(Surv(t.obs, delta) ~ X + Z, data = dados_imp)
m.cox[[i]] <- models
}
m.cox
betas_hat <- list()
# agora vamos obter as estimativas de beta e OMEGA
betas_hat <- list()
# agora vamos obter as estimativas de beta e OMEGA
betas_hat <- list()
for(i in seq_along(m.cox)){
fit <- m.cox[[i]]
coefs <- coef(fit)
betas_hat[[i]] <- coefs
}
betas_hat
OMEGA_hat <- list()
OMEGA_hat <- list()
for(i in seq_along(m.cox)){
fit <- m.cox[[i]]
matriz <- vcov(fit)
OMEGA_hat <- matriz
}
OMEGA_hat
OMEGA_hat <- list()
for(i in seq_along(m.cox)){
fit <- m.cox[[i]]
matriz <- vcov(fit)
OMEGA_hat[[i]] <- matriz
}
OMEGA_hat
# Agora vamos obter os betas_tilde's da normal multivariada
betas_tilde <- list()
# Agora vamos obter os betas_tilde's da normal multivariada
betas_tilde <- list()
for(i in seq_along(m.cox)){
betas_tilde[[i]] <- MASS::mvrnorm(n = 1, mu = betas_hat[[i]], Sigma = OMEGA_hat[[i]])
}
betas_tilde
betas_tilde
S_t <- list()
betas_tilde
betas_tilde[[1]]
betas_tilde[[1]][1]
S_t[[i]] <- exp(betas_tilde[[i]][1]*imp_0$X[[i]] + betas_tilde[[i]][2]*imp_0$Z[[i]])
S_t <- list()
for(i in seq_along(betas_tilde)){
S_t[[i]] <- exp(betas_tilde[[i]][1]*imp_0$X[[i]] + betas_tilde[[i]][2]*imp_0$Z[[i]])
}
S_t[[i]]
S_t <- list()
for(i in seq_along(betas_tilde)){
S_t[[i]] <- exp(betas_tilde[[i]][1]*imp_0$X[[i]] + betas_tilde[[i]][2]*imp_0$Z[[i]])
}
S_t
S_t <- list()
for(i in seq_along(betas_tilde)){
S_t[[i]] <- exp(betas_tilde[[i]][1]*imp_0$X[[i]] + betas_tilde[[i]][2]*imp_0$Z[[i]])
}
S_t
S_t[[i]] <- exp(betas_tilde[[i]][1] + betas_tilde[[i]][2])
S_t <- list()
for(i in seq_along(betas_tilde)){
S_t[[i]] <- exp(betas_tilde[[i]][1] + betas_tilde[[i]][2])
}
S_t
imp_0
imp_0[[1]]["X"]
S_t <- list()
for(i in seq_along(betas_tilde)){
S_t[[i]] <- exp(betas_tilde[[i]][1]*imp_0[[i]]["X"]
+ betas_tilde[[i]][2]*imp_0[[1]]["Z"]
)
}
S_t
H0_t <- -log(S_t)
lapply(m.cox, FUN = vcov)
OMEGA_hat
H0_t <- lapply(S_t, -log)
H0_t <- lapply(S_t, -log())
H0_t <- lapply(S_t, FUN = -log())
H0_t <- list()
H0_t <- list()
for(i in seq_along(S_t)){
H0_t[[i]] <- -log(S_t[[i]])
}
H0_t
H0_t[[1]]
plot(H0_t[[1]])
H0_t <- list()
H0_t <- list()
for(i in seq_along(m.cox))
H0_t <- list()
for(i in seq_along(m.cox)){
fit <- m.cox[[i]]
H0_t[[i]] <- basehaz(fit)
}
H0_t
plot(H0_t)
plot([H0_t[[i]])
plot([H0_t[[1]])
plot(H0_t[[1]])
plot(H0_t[[1]][,"hazard"])
H0_t[[]][,"hazard"]
H0_t[,"hazard"]
H0_t[[1]][,"hazard"]
H0_t
H0_t <- list()
for(i in seq_along(m.cox)){
fit <- m.cox[[i]]
H0_t[[i]] <- basehaz(fit)[,1]
}
H0_t
?basehaz()
install.packages("basehaz.gbm")
library(muhaz)
muhaz(imp_0[[1]]["t.obs"], imp_0[[1]]["delta"])
imp_0[[1]]
imp_0[[1]]["t.obs"]
imp_0[[2]]["delta"]
cbind(imp_0[[1]]["t.obs"],imp_0[[2]]["delta"])
tb <- as.data.frame(cbind(imp_0[[1]]["t.obs"],imp_0[[2]]["delta"]))
tb
?muhaz
muhaz(times = tb$t.obs, delta = tb$delta)
h0_t <- muhaz(times = tb$t.obs, delta = tb$delta)
h0_t
h0_t$haz.est
plot(h0_t$haz.est)
setwd("C:/Users/BIANCA/OneDrive/Documentos/PPGMNE - Transversais/MEPC 2024/mepc-exams-2024")
setwd("C:/Users/BIANCA/OneDrive/Documentos/PPGMNE - Transversais/MEPC 2024/mepc-exams-2024/semana11-revisao-extensoes")
dados <- data.frame(x = c(10.00, 8.00, 13.00, 9.00, 11.00, 14.00, 6.00, 4.00, 12.00, 7.00, 5.00),
y =  c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68))
head(dados)
m1 <- glm(y ~ x, family = "gaussian", data = dados)
summary(dados)
summary(m1)
# Qual a estimatimativa do intercepto do modelo?
coef(m1)$intercept
# Qual a estimatimativa do intercepto do modelo?
summary(m1)$coef
# Qual a estimatimativa do intercepto do modelo?
summary(m1)$coef[1,1]
# Qual a estimatimativa do intercepto do modelo?
summary(m1)$coef[,1]
# Qual a estimatimativa do intercepto do modelo?
summary(m1)$coef[1,1]
# Qual a estimatimativa do intercepto do modelo?
round(summary(m1)$coef[1,1], 3)
# Qual a estimatimativa do intercepto do modelo?
summary(m1)$coef[1,1]
# Qual a estimatimativa do intercepto do modelo?
round(summary(m1)$coef[1,1],3)
# Qual o erro padrão da variável x?
round(summary(m1)$coef[2,2], 3)
# Qual o valor do AIC?
m1$aic
# Qual o valor do AIC?
round(m1$aic,3)
# Qual o valor do parâmetro de dispersão?
m1$df.residual
m1$effects
m1$deviance
# Qual a estimatimativa do intercepto do modelo?
round(summary(m1)$coef[1,1],3)
# Qual o erro padrão da variável x?
round(summary(m1)$coef[2,2], 3)
# Qual o valor do AIC?
round(m1$aic,3)
# Qual o valor da Deviance Residual?
m1$deviance
# Qual o valor do parâmetro de dispersão?
summary(m1)$dispersion
vet_desc <- format(dados, ndigits = 2)
vet_desc <- vet_desc
# Criando o dataframe
dados <- data.frame(x = c(10.00, 8.00, 13.00, 9.00, 11.00, 14.00, 6.00, 4.00, 12.00, 7.00, 5.00),
y = c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68))
vet_desc <- format(dados, ndigits = 2)
vet_desc
# A deviance residual é uma medida de quão bem o modelo ajusta os dados observados, representando a soma dos resíduos ao quadrado.
# Qual é o valor da deviance residual do modelo ajustado?
round(m1$deviance,2)
require(knitr)
rm(list=ls())
# Criando o dataframe
dados <- data.frame(x = c(10.00, 8.00, 13.00, 9.00, 11.00, 14.00, 6.00, 4.00, 12.00, 7.00, 5.00),
y = c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68))
# jAustando o modelo GLM
m1 <- glm(y ~ x, family = "gaussian", data = dados)
# Resumo do modelo
summary(m1)
i <- 0
i <- i+1
termo[i] <- "A estimativa do intercepto do modelo representa o valor esperado de y quando x é igual a zero.
# Qual é a estimativa do intercepto do modelo?"
# Qual é a estimativa do intercepto do modelo?"
conceito[i] <- round(summary(m1)$coef[1, 1], 3)
intercepto <- round(summary(m1)$coef[1, 1], 3)
# Extraindo informações do modelo
intercepto <- round(summary(m1)$coef[1, 1], 3)
x_se <- round(summary(m1)$coef[2, 2], 3)
aic <- round(m1$aic, 3)
deviance_residual <- m1$deviance
disp_parametro <- summary(m1)$dispersion
deviance_nula <- summary(m1)$null.deviance
# Criando um dataframe com as informações
df <- data.frame(
intercepto = intercepto,
x_se = x_se,
aic = aic,
deviance_residual = deviance_residual,
disp_parametro = disp_parametro,
deviance_nula = deviance_nula
)
setwd("~/mepc-exams-2024/nao-avaliativas/semana11-revisao-extensoes")
exams::exams2html("1-saida-modelo-associa.Rmd")
exams::exams2html("2-pred.Rmd")
exams::exams2html("2-pred.Rmd")
exams::exams2html("2-pred.Rmd")
exams::exams2html("2-pred.Rmd")
setwd("~/mepc-exams-2024/nao-avaliativas/semana11-revisao-extensoes")
files <- dir()
files <- dir()
for (i in 1:length(files)) {
exams::exams2moodle(files[i], n=10, name = files[i])
}
setwd("~/mepc-exams-2024/nao-avaliativas/semana12-revisao-regressao-planejamento")
files <- dir()
for (i in 1:length(files)) {
exams::exams2moodle(files[i], n=10, name = files[i])
}
files <- dir()
files
files <- dir()
for (i in 1:length(files)) {
exams::exams2moodle(files[i], n=10, name = files[i])
}
install.packages("agridat")
files <- dir()
for (i in 1:length(files)) {
exams::exams2moodle(files[i], n=10, name = files[i])
}
y=c(7,9,10,13,14,15)
p <- c(0.1,0.1,0.1,0.1,0.2,0.4)
y=c(7,9,10,13,14,15)
p=c(0.1,0.1,0.1,0.1,0.2,0.4)
y=c(7,9,10,13,14,15)
p=c(0.1,0.1,0.1,0.1,0.2,0.4)
sum(y*p)
sum(y*p)
(y-sum(y*p))^2*p
sum((y-sum(y*p))^2*p)
sqrt(sum((y-sum(y*p))^2*p))
knitr::opts_chunk$set(echo = TRUE, eval = T)
knitr::kable(data.frame(
Resposta = c(paste0('P(Y > c) = 0,01'),
paste0('c = ', round(qnorm(0.99,70,11),2)))
)
)
setwd("~/CE3012024")
rmarkdown::render_site()
